optimizer:
  _target_: torch.optim.AdamW
  lr: 0.01
  amsgrad: true
  weight_decay: 0.001
scheduler:
  _target_: torch.optim.lr_scheduler.StepLR
  step_size: 5
  gamma: 0.5
model:
  _target_: tr_learn.model.cls_model.PlateClassification
  is_full_train: false
  model_type: resnet18
datamodule:
  _target_: tr_learn.data.datamodule.PlateDataModuleTrainValid
  train_load_info:
    _target_: tr_learn.data.datamodule.LoadInfo
    is_jit_transform: true
    root: ./data/raw/plates
    batch_size: 8
    transforms:
    - _target_: torch.nn.Sequential
      _args_:
      - _target_: torchvision.transforms.v2.RandomResizedCrop
        _convert_: partial
        size:
        - 224
        interpolation:
          _target_: tr_learn.data.transforms.to_interpolation_mode
          mode: BILINEAR
        scale:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.5
          - 1.0
        ratio:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.9
          - 1.2
        antialias: true
      - _target_: tr_learn.data.transforms.convert_dtype
      - _target_: torchvision.transforms.v2.Normalize
        _convert_: partial
        mean:
        - 0.485
        - 0.456
        - 0.406
        std:
        - 0.229
        - 0.224
        - 0.225
    - _target_: torch.nn.Sequential
      _args_:
      - _target_: torchvision.transforms.v2.RandomVerticalFlip
        p: 1.0
      - _target_: torchvision.transforms.v2.RandomResizedCrop
        _convert_: partial
        size:
        - 224
        interpolation:
          _target_: tr_learn.data.transforms.to_interpolation_mode
          mode: BILINEAR
        scale:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.5
          - 1.0
        ratio:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.9
          - 1.2
        antialias: true
      - _target_: tr_learn.data.transforms.convert_dtype
      - _target_: torchvision.transforms.v2.Normalize
        _convert_: partial
        mean:
        - 0.485
        - 0.456
        - 0.406
        std:
        - 0.229
        - 0.224
        - 0.225
    - _target_: torch.nn.Sequential
      _args_:
      - _target_: torchvision.transforms.v2.RandomResizedCrop
        _convert_: partial
        size:
        - 224
        interpolation:
          _target_: tr_learn.data.transforms.to_interpolation_mode
          mode: BILINEAR
        scale:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.5
          - 1.0
        ratio:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.9
          - 1.2
        antialias: true
      - _target_: torchvision.transforms.v2.ColorJitter
        hue: 0.5
        saturation: 0.25
        contrast: 0.5
        brightness: 0.5
      - _target_: tr_learn.data.transforms.convert_dtype
      - _target_: torchvision.transforms.v2.Normalize
        _convert_: partial
        mean:
        - 0.485
        - 0.456
        - 0.406
        std:
        - 0.229
        - 0.224
        - 0.225
    - _target_: torch.nn.Sequential
      _args_:
      - _target_: torchvision.transforms.v2.RandomHorizontalFlip
        p: 0.6
      - _target_: torchvision.transforms.v2.RandomVerticalFlip
        p: 0.6
      - _target_: torchvision.transforms.v2.RandomResizedCrop
        _convert_: partial
        size:
        - 224
        interpolation:
          _target_: tr_learn.data.transforms.to_interpolation_mode
          mode: BILINEAR
        scale:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.5
          - 1.0
        ratio:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.9
          - 1.2
        antialias: true
      - _target_: torchvision.transforms.v2.ColorJitter
        hue: 0.5
        saturation: 0.25
        contrast: 0.5
        brightness: 0.5
      - _target_: tr_learn.data.transforms.convert_dtype
      - _target_: torchvision.transforms.v2.Normalize
        _convert_: partial
        mean:
        - 0.485
        - 0.456
        - 0.406
        std:
        - 0.229
        - 0.224
        - 0.225
    - _target_: torch.nn.Sequential
      _args_:
      - _target_: torchvision.transforms.v2.RandomRotation
        degrees: 45
        interpolation:
          _target_: tr_learn.data.transforms.to_interpolation_mode
          mode: BILINEAR
      - _target_: torchvision.transforms.v2.RandomResizedCrop
        _convert_: partial
        size:
        - 224
        interpolation:
          _target_: tr_learn.data.transforms.to_interpolation_mode
          mode: BILINEAR
        scale:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.5
          - 1.0
        ratio:
          _target_: tr_learn.data.transforms.to_tuple
          items:
          - 0.9
          - 1.2
        antialias: true
      - _target_: tr_learn.data.transforms.convert_dtype
      - _target_: torchvision.transforms.v2.Normalize
        _convert_: partial
        mean:
        - 0.485
        - 0.456
        - 0.406
        std:
        - 0.229
        - 0.224
        - 0.225
    num_workers: 0
  valid_load_info:
    _target_: tr_learn.data.datamodule.LoadInfo
    root: ./data/raw/plates
    is_jit_transform: true
    batch_size: 8
    transforms:
    - _target_: torch.nn.Sequential
      _args_:
      - _target_: torchvision.transforms.v2.Resize
        interpolation:
          _target_: tr_learn.data.transforms.to_interpolation_mode
          mode: BILINEAR
        _convert_: partial
        antialias: true
        size:
        - 224
      - _target_: torchvision.transforms.v2.CenterCrop
        size: 224
      - _target_: tr_learn.data.transforms.convert_dtype
      - _target_: torchvision.transforms.v2.Normalize
        _convert_: partial
        mean:
        - 0.485
        - 0.456
        - 0.406
        std:
        - 0.229
        - 0.224
        - 0.225
    num_workers: 0
  predict_load_info:
    _target_: tr_learn.data.datamodule.LoadInfo
    root: ./data/raw/plates
    is_jit_transform: true
    batch_size: 8
    transforms:
    - _target_: torch.nn.Sequential
      _args_:
      - _target_: torchvision.transforms.v2.Resize
        interpolation:
          _target_: tr_learn.data.transforms.to_interpolation_mode
          mode: BILINEAR
        _convert_: partial
        antialias: true
        size:
        - 224
      - _target_: torchvision.transforms.v2.CenterCrop
        size: 224
      - _target_: tr_learn.data.transforms.convert_dtype
      - _target_: torchvision.transforms.v2.Normalize
        _convert_: partial
        mean:
        - 0.485
        - 0.456
        - 0.406
        std:
        - 0.229
        - 0.224
        - 0.225
    num_workers: 4
transforms:
  image_norm_mean:
  - 0.485
  - 0.456
  - 0.406
  image_norm_std:
  - 0.229
  - 0.224
  - 0.225
  normalize_transform:
  - _target_: tr_learn.data.transforms.convert_dtype
  - _target_: torchvision.transforms.v2.Normalize
    _convert_: partial
    mean:
    - 0.485
    - 0.456
    - 0.406
    std:
    - 0.229
    - 0.224
    - 0.225
  resize_crop_transform:
    _target_: torchvision.transforms.v2.RandomResizedCrop
    _convert_: partial
    size:
    - 224
    interpolation:
      _target_: tr_learn.data.transforms.to_interpolation_mode
      mode: BILINEAR
    scale:
      _target_: tr_learn.data.transforms.to_tuple
      items:
      - 0.5
      - 1.0
    ratio:
      _target_: tr_learn.data.transforms.to_tuple
      items:
      - 0.9
      - 1.2
    antialias: true
  augment_transforms:
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomVerticalFlip
      p: 1.0
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: torchvision.transforms.v2.ColorJitter
      hue: 0.5
      saturation: 0.25
      contrast: 0.5
      brightness: 0.5
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomHorizontalFlip
      p: 0.6
    - _target_: torchvision.transforms.v2.RandomVerticalFlip
      p: 0.6
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: torchvision.transforms.v2.ColorJitter
      hue: 0.5
      saturation: 0.25
      contrast: 0.5
      brightness: 0.5
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomRotation
      degrees: 45
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  train_transform:
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomVerticalFlip
      p: 1.0
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: torchvision.transforms.v2.ColorJitter
      hue: 0.5
      saturation: 0.25
      contrast: 0.5
      brightness: 0.5
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomHorizontalFlip
      p: 0.6
    - _target_: torchvision.transforms.v2.RandomVerticalFlip
      p: 0.6
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: torchvision.transforms.v2.ColorJitter
      hue: 0.5
      saturation: 0.25
      contrast: 0.5
      brightness: 0.5
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.RandomRotation
      degrees: 45
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
    - _target_: torchvision.transforms.v2.RandomResizedCrop
      _convert_: partial
      size:
      - 224
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      scale:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.5
        - 1.0
      ratio:
        _target_: tr_learn.data.transforms.to_tuple
        items:
        - 0.9
        - 1.2
      antialias: true
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  valid_transform:
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.Resize
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      _convert_: partial
      antialias: true
      size:
      - 224
    - _target_: torchvision.transforms.v2.CenterCrop
      size: 224
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
  test_transform:
  - _target_: torch.nn.Sequential
    _args_:
    - _target_: torchvision.transforms.v2.Resize
      interpolation:
        _target_: tr_learn.data.transforms.to_interpolation_mode
        mode: BILINEAR
      _convert_: partial
      antialias: true
      size:
      - 224
    - _target_: torchvision.transforms.v2.CenterCrop
      size: 224
    - _target_: tr_learn.data.transforms.convert_dtype
    - _target_: torchvision.transforms.v2.Normalize
      _convert_: partial
      mean:
      - 0.485
      - 0.456
      - 0.406
      std:
      - 0.229
      - 0.224
      - 0.225
logger:
  _target_: lightning.pytorch.loggers.TensorBoardLogger
  save_dir: ./exp/cls_training/logs
loss:
  _target_: torch.nn.BCEWithLogitsLoss
  reduction: none
exp_dir: ./exp/cls_training
seed: 172197
trainer:
  _target_: lightning.Trainer
  accelerator: gpu
  precision: '32'
  fast_dev_run: false
  max_epochs: 20
  check_val_every_n_epoch: 1
  log_every_n_steps: 10
  benchmark: true
  deterministic: false
  inference_mode: true
  logger:
  - _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: ./exp/cls_training/logs
  callbacks:
  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: epoch
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    save_top_k: 2
    mode: max
    monitor: Valid/accuracy
    dirpath: ./exp/cls_training/checkpoints
    filename: '{epoch}-{Valid/accuracy:.3f}'
    save_last: true
    save_weights_only: true
    every_n_epochs: 1
